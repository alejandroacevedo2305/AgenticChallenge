"""
Evaluation Configuration for SOC Agent Benchmarks.

This file contains configuration settings and evaluation criteria
for the AI Junior SOC Analyst agent benchmarks.
"""

from dataclasses import dataclass, field
from typing import Dict, List, Any


@dataclass
class EvaluationConfig:
    """Configuration settings for SOC agent evaluation."""

    # Minimum thresholds for passing evaluation
    MIN_DETECTION_RATE: float = 80.0  # Minimum % of malicious IPs to detect
    MIN_REPORT_QUALITY: float = 7.0  # Minimum report quality score (1-10)
    MIN_RESPONSE_TIME: float = 30.0  # Maximum acceptable response time in seconds

    # LLM-as-a-Judge settings
    JUDGE_MODEL: str = (
        "ollama:llama3.1:8b"  # Local Ollama model for evaluation (note: ollama: prefix)
    )
    JUDGE_BASE_URL: str = "http://localhost:11434"  # Ollama service URL
    JUDGE_TEMPERATURE: float = 0.1  # Low temperature for consistent scoring

    # Evaluation criteria weights
    CRITERIA_WEIGHTS: Dict[str, float] = field(
        default_factory=lambda: {
            "detection_accuracy": 0.4,  # 40% weight on correct threat detection
            "report_quality": 0.3,  # 30% weight on report quality
            "response_time": 0.2,  # 20% weight on performance
            "false_positive_rate": 0.1,  # 10% weight on minimizing false positives
        }
    )


# Evaluation prompts for LLM-as-a-Judge
REPORT_QUALITY_PROMPT = """
You are evaluating a cybersecurity incident report generated by an AI SOC analyst.

Please rate the report on a scale of 1-10 based on the following criteria:

1. **Clarity and Structure** (25%): Is the report well-organized and easy to understand?
2. **Technical Accuracy** (25%): Are the technical details correct and appropriate?
3. **Actionability** (25%): Does the report provide clear, actionable recommendations?
4. **Completeness** (25%): Does the report cover all important aspects of the incident?

Report to evaluate:
---
{report_content}
---

Please provide:
1. A score from 1-10 for each criterion
2. An overall score from 1-10
3. Brief justification for your scoring
4. Specific suggestions for improvement

Format your response as JSON:
{{
    "clarity_score": <1-10>,
    "technical_accuracy_score": <1-10>,
    "actionability_score": <1-10>,
    "completeness_score": <1-10>,
    "overall_score": <1-10>,
    "justification": "<explanation>",
    "improvements": ["<suggestion1>", "<suggestion2>"]
}}
"""

THREAT_ANALYSIS_PROMPT = """
You are evaluating the threat analysis capabilities of an AI SOC analyst.

Given the following log data and the agent's analysis, assess:

1. **Detection Accuracy**: Did the agent correctly identify suspicious activities?
2. **Threat Classification**: Are the identified threats properly categorized?
3. **Risk Assessment**: Is the risk level appropriately assigned?
4. **Context Understanding**: Does the agent understand the broader security context?

Log data:
---
{log_data}
---

Agent's analysis:
---
{agent_analysis}
---

Ground truth threats:
---
{ground_truth}
---

Provide a detailed evaluation in JSON format:
{{
    "detection_accuracy": <1-10>,
    "threat_classification": <1-10>,
    "risk_assessment": <1-10>,
    "context_understanding": <1-10>,
    "missed_threats": ["<list of missed threats>"],
    "false_positives": ["<list of false positives>"],
    "overall_assessment": "<detailed evaluation>"
}}
"""


# Test cases for comprehensive evaluation
EVALUATION_TEST_CASES = [
    {
        "name": "basic_brute_force_detection",
        "description": "Test detection of SSH brute force attacks",
        "input_file": "data/auth.log",
        "expected_threats": ["203.0.113.55", "192.0.2.147"],
        "difficulty": "easy",
    },
    {
        "name": "advanced_pattern_recognition",
        "description": "Test recognition of complex attack patterns",
        "input_file": "data/auth.log",
        "expected_patterns": ["credential_stuffing", "reconnaissance"],
        "difficulty": "medium",
    },
    {
        "name": "report_generation_quality",
        "description": "Test quality of generated incident reports",
        "focus": "report_quality",
        "min_score": 7.0,
        "difficulty": "medium",
    },
]


def get_evaluation_config() -> EvaluationConfig:
    """Get the default evaluation configuration."""
    return EvaluationConfig()


def get_judge_prompts() -> Dict[str, str]:
    """Get all LLM-as-a-judge prompts."""
    return {
        "report_quality": REPORT_QUALITY_PROMPT,
        "threat_analysis": THREAT_ANALYSIS_PROMPT,
    }


def get_test_cases() -> List[Dict[str, Any]]:
    """Get all evaluation test cases."""
    return EVALUATION_TEST_CASES
